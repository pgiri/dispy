<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
  <head> 
    <meta http-equiv="content-type" content="text/html; charset=utf-8" /> 
    <meta name="description" content="Python module for distributing computations across multiple processors on a single machine, among many machines in a cluster or grid. The computations can be standalone programs or python functions." /> 
    <meta name="keywords" content="dispy, python, parallel processing, distributed computing, cluster computing" /> 
    <title> 
      dispy - Distribute Computations for Parallel Execution
    </title> 

<link rel="stylesheet" type="text/css" href="style.css" />
 
  </head> 
  <body> 
    <center><h4>dispy : Distribute Computations for Parallel Execution</h4></center>
    <div id="content-mask">
      <div id="content-container"> 
	<div id="content-menu">
	  <ul>
            <li><a href="index.html">Project Page</a></li> 
            <li><a href="asyncoro.html">asyncoro</a></li>
            <li><a href="dispynode.html">dispynode</a></li> 
            <li><a href="dispyscheduler.html">dispyscheduler</a></li> 
            <li><a href="dispynetrelay.html">dispynetrelay</a></li>
	  </ul>
	  <hr />
	  <ul>
            <li><a href="http://sourceforge.net/projects/dispy/files">Download dispy</a></li> 
            <li><a href="http://sourceforge.net/projects/dispy/">Project details</a></li> 
            <li><a href="http://sourceforge.net/projects/dispy/support">Get support</a></li>
	  </ul>
	</div> 

	<div id="content-wrap"> 
	  <div id="content"> 
        <center><h2>dispy</h2></center>
<p>
  There are two ways to create clusters with
  dispy: <tt>JobCluster</tt> and <tt>SharedJobCluster</tt>. If only
  one instance of dispy may be running at anytime,
  <tt>JobCluster</tt> is simple to use; it already contains a
  scheduler that will schedule jobs to nodes running 'dispynode'. If,
  however, multiple programs using dispy may be running at anytime,
  <tt>JobCluster</tt> cannot be used - each of the schedulers in each
  instance of dispy will assume the nodes are controlled exclusively
  by each, causing conflicts. Instead, <tt>SharedJobCluster</tt> must
  be used. In this
  case, <a href="dispyscheduler.html">dispyscheduler</a> must also be
  running on some computer and <tt>SharedJobCluster</tt> must be
  initialized with the node running dispyscheduler (default is the
  host that calls <tt>SharedJobCluster</tt>).
  </p>

<p>Once an instance of JobCluster/SharedJobCluster is created, it can
be used to schedule jobs by specifying arguments to invoke the
computations.
</p>

<h4>JobCluster</h4>
<tt>JobCluster(computation, nodes=['*'], depends=[], callback=None,
           ip_addr=None, ext_ip_addr=None, port=51347, node_port=51348,
           fault_recover=False, dest_path=None, loglevel=logging.WARNING,
           cleanup=True, pulse_interval=None, ping_interval=None,
           reentrant=False, secret='', keyfile=None, certfile=None)</tt><br/>
where
<ul>
<li>
  <tt>computation</tt> is either name of Python function or a
  string. If it is a string, it must be path to executable program. This
  computation is sent to nodes in the given cluster.
</li>
<li>
  <tt>nodes</tt> is list. Each element must be either a string or a
  pair (tuple of two elements). If element is a string, it must be
  either IP address or host name. If element is a pair, first element
  of pair must be IP address or name and second element must be port
  number where that node is serving (needed if it is different from
  default 51348). This list serves two purposes: dispy initially sends
  a request to all the nodes listed to find out information about them
  (e.g., number of processing units available for dispy), then sends
  given computation to only those nodes that match the listed nodes
  (dispy may know about nodes not listed in this computation, as it
  also broadcasts identification request).  Wildcard '*' can be used
  to match (part of) any IP address; e.g., '192.168.3.*' matches any
  node whose IP address starts with '192.168.3'.  If there are any
  nodes beyond local network, then all such nodes should be mentioned
  in <tt>nodes</tt>. If there are many such nodes (on outside local
  network), it may be cumbersome to list them all (it is not possible
  to send identification request to outside networks with wildcard in
  them); in that case, <a href="dispynetrelay.html">dispynetrelay</a>
  may be started on one of the nodes on that network and the node
  running dispynetrelay should be added to nodes list (and a wildcard
  for that network, so that other nodes on that network match that
  wildcard); see below for examples.
</li>
<li>
  <tt>depends</tt> is list of dependencies needed
  for <tt>computation</tt>. Each element of this list can be either
  Python function or Python class or an instance of class (object) or
  a Python module or path to file. Only Python modules that are not
  present on nodes already need be listed; standard modules that are
  present on all nodes do not need to be listed here.
</li>
<li>
  <tt>callback</tt> is a function. When a job's results become
  available, dispy will call provided callback function with that job
  as the argument. If a job sends provisional results with
  'dispy_provisional_result' multiple times, then dispy will call
  provided callback each such time. The (provisional) results of
  computation can be retrieved with 'result' field of job, etc. While
  computations are run on nodes in isolated environments, callbacks
  are run in the context of user programs from which
  (Shared)JobCluster is called - for example, callbacks can access
  global variables in programs that created cluster(s).
</li>
<li>
  <tt>ip_addr</tt> is IP address to use for (client)
  communication. This may be needed in case the client has multiple
  interfaces and default interface is not the right choice.
</li>
<li>
  <tt>ext_ip_addr</tt> is external IP address to use for (client)
  communication. This may be needed in case the client is behind a NAT
  firewall/gateway and (some) nodes are outside. Typically, in such a
  case, ext_ip_addr must be the address of NAT firewall/gateway and
  the NAT firewall/gateway must forward ports to ip_addr
  appropriately. See below for more information.
</li>
<li>
  <tt>port</tt> is port to use for (client) communication. Usually not
  necessary. If not given, dispy will request socket library to choose
  any available port.
</li>
<li>
  <tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this is different from default, 'dispynode' programs
  must be run with the same port.
</li>
<li>
  <tt>dest_path</tt> is directory on the nodes where files are
  transferred to. Default is to create a separate directory for each
  computation. If a computation transfers files (dependencies) and
  same computation is run again with same files, the transfer can be
  avoided by specifying same dest_path, along with the option
  'cleanup=False'.
</li>
<li>
  <tt>fault_recover</tt> must be either True or a string. If it is a
  string, dispy uses it as path to file where it stores information
  about jobs scheduled but not finished yet. In case user program
  terminates unexpectedly (for example, because of network failure,
  uncaught exception), the results of submitted jobs can be later
  retrieved through 'fault_recover_jobs' function (see below). If this
  option is True, dispy will store information about jobs in a file of
  the form '_dispy_fault_recover_YYYYMMDDHHMMSS' in the current
  directory. Note that dispy keeps information about only the jobs
  that have been submitted for execution but not finished yet. Once a
  job is finished (i.e., job result is received by dispy's scheduler),
  its information is lost. If it is necessary to keep the information
  about finished jobs, callbacks can be used to persist job results.
</li>
<li>
  <tt>loglevel</tt> is message priority for logging module.
</li>
<li>
  <tt>cleanup</tt>: Whether any files transferred should be deleted
  after the computation is done. If it is False, the files are left on
  the nodes; this may speedup if same files are needed for another
  cluster later. However, this can be security risk and/or require
  manual cleanup. If same files are used for multiple clusters, then
  cleanup may be set to False and same <tt>dest_path</tt> used.
</li>
<li>
  <tt>pulse_interval</tt> is number of seconds between 'pulse'
  messages that nodes send to indicate they are alive and computing
  submitted jobs. If this value is given as an integer or floating
  number between 1 and 600, then a node is presumed dead if
  5*pulse_interval seconds elapse without a pulse message. See
  'reentrant' below.
</li>
<li>
  <tt>reentrant</tt> must be either True or False. This value is used
  only if 'pulse_interval' is set for any of the clusters. If
  pulse_interval is given and reentrant is False (default), jobs
  scheduled for a dead node are automatically cancelled (for such jobs,
  execution result, output and error fields are set to None, exception
  field is set to 'Cancelled' and status is set to Cancelled); if
  reentrant is True, then jobs scheduled for a dead node are resubmitted
  to other available nodes.
</li>
<li>
  <tt>ping_interval</tt> is number of seconds.  Normally dispy can
  locate nodes running dispynode by broadcasting UDP ping messages on
  local network and point-to-point UDP messages to nodes on remote
  networks. However, UDP messages may get lost.  Ping interval is
  number of seconds between repeated ping messages to find any nodes
  that have missed previous ping messages.
  </li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes. This prevents unauthorized
  use of nodes. However, the hashed string (not the secret itself) is
  passed in clear text, so an unauthorized, determined person may be
  able to figure out how to circumvent.
</li>
<li>
  <tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None.
</li>
<li>
  <tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module).
</li>
</ul>

<h4>SharedJobCluster</h4>
<p>
  SharedJobCluster has almost the same syntax, except as noted below.
  </p>
<tt>SharedJobCluster(computation, nodes=['*'], depends=[], ip_addr=None,
                 ext_ip_addr=None, scheduler_node=None, port=51347,
                 dest_path=None, loglevel=logging.WARNING, cleanup=True,
                 reentrant=False, secret='', keyfile=None, certfile=None)</tt><br/>
where all arguments common to <tt>JobCluster</tt> are same, and
<ul>
<li>
  <tt>scheduler_node</tt> is either IP address or host name
  where <a href="dispyscheduler.html">dispyscheduler</a> is running; if
  it is not given, the node where SharedJobCluster is invoked is
  used
</li>
<li>
  <tt>pulse_interval</tt> is not used in case of SharedJobCluster;
  instead, 'dispyscheduler' must be called with '--pulse_interval'
  option appropriately.
</li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with dispyscheduler.
</li>
</ul>

A cluster has following methods:
<ul>
<li>
  <tt>cluster.submit</tt> method is available for the cluster returned
  from JobCluster. This method should be called with the arguments
  exactly as expected by the 'computation' given to JobCluster. If
  'computation' is a Python function, the arguments may also contain
  keyword arguments. However, all arguments must be serializable
  (picklable). If an argument is a class object that contains
  non-serializable members, then the classes may
  provide <tt>__getstate__</tt> method for this purpose (see '_Job'
  class in dispy.py for an example). If 'computation' is a program, then
  all arguments must be strings.

  <tt>submit</tt> returns a 'job' object. Results from execution of
  computation with given arguments will be available in the 'job' object
  after execution finishes.
</li>

<li>
  <tt>cluster()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.wait()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.close()</tt> will wait for all submitted jobs to complete
  and then cleanup (such as removing any transferred files, deleting
  'computation' from the nodes etc.).
</li>

<li>
  <tt>cluster.cancel(job)</tt> will remove the job submitted, by
  terminating it if it already started execution. Note that if the job
  execution has any side effects (such as updating database, files
  etc.), cancelling a job may leave unpredictable side effects,
  depending on at what point job is cancelled.
</li>

<li>
  <tt>cluster.stats()</tt> will print statistics about nodes, time
  each node spent executing jobs etc.
</li>
</ul>

<h4>Fault Recovery</h4>
<p>
As noted above, if 'fault_recover' option is used when creating a
cluster, dispy stores information about scheduled but unfinished jobs
in a file. If user program then terminates unexpectedly, the nodes
that execute those jobs can't send the results back to dispy. In such
cases, the results for the jobs can be retrieved from the nodes with
the function in dispy</p>
<tt>fault_recover_jobs(fault_recover_file, ip_addr=None, secret='', node_port=51348,
                       certfile=None, keyfile=None)</tt>
where
<ul>
  <li><tt>fault_recover_file</tt> is path to the file used or created
  when JobCluster is used.</li>
  <li><tt>ip_addr</tt> is IP address to use for (client)
    communication. This may be needed in case the client has multiple
    interfaces and default interface is not the right choice (this
    would be same as the 'ip_addr' option used for JobCluster).</li>
  <li><tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes (should same as the one used
  when creating JobCluster).</li>
  <li><tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this should be different from default, 'dispynode'
  programs must be run with the same port. This option should be same
  as the one used when creating JobCluster.</li>
  <li><tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module) (same as the one used when creating JobCluster).</li>
  <li><tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None. This option
  should be same as the one used when creating JobCluster.</li>
</ul>

<p>
This function reads the information about jobs in the
fault_recover_file, retrieves DispyJob instance (that contains
results, stdout, stderr, status etc.) for each job that was scheduled
for execution but unfinished at the time of crash, and returns them as
a list. If a job has finished executing at the time
'fault_recover_jobs' function is called, the information about that is
deleted from both the node and fault_recover_file, so the results for
finished jobs can't be retrieved more than once. However, if a job is
still executing, the status field of DispyJob would be
DispyJob.Running and the results for this job can be retrieved again
(until that job finishes) by calling 'fault_recover_jobs'. Note that
'fault_recover_jobs' is available as separate function - it doesn't
need JobCluster or SharedJobCluster instance. In fact,
'fault_recover_jobs' function must not be used when a cluster that
uses same recover file is currently running.
</p>

<p>
Note that dispy sends only the given computation and its dependencies
to the nodes; the program itself is not transferred. So if computation
is a python function, it must import all the modules used by it, even
if the program imported those modules before cluster is created.
</p>

<h4>Provisional Results</h4>
<p>
  'dispy_provisional_result' function can be used in computations
  (Python functions) to send provisional results back to the
  client. For example, in optimization computations, there may be many
  (sub) optimal results that the computations can inform the client
  (program) that may cancel computations, or create additional
  computations, etc. 'dispy_provisional_result' can be used to send
  any information, any number of times, back to the client. As an
  illustrative example, consider:
  </p>
<code>#!/usr/bin/env python

import random, dispy

def compute(n, threshold):
    import random, time, socket
    name = socket.gethostname()
    for i in xrange(0, n):
        r = random.uniform(0, 1)
        if r <= threshold:
            # possible result
            dispy_provisional_result((name, r))
        time.sleep(0.1)
    # final result
    return None

def job_callback(job):
    # callback is called for Terminated status, too, in which case
    # job.result would be None
    if job.result is not None:
        if job.result[1] < 0.005:
            print '%s computed: %s' % (job.result[0], job.result[1])
            global jobs, cluster
            for j in jobs:
                if j.status == dispy.DispyJob.ProvisionalResult:
                    cluster.cancel(j)

if __name__ == '__main__':
    cluster = dispy.JobCluster(compute, callback=job_callback)
    jobs = []
    for n in xrange(4):
        job = cluster.submit(random.randint(50,100), 0.2)
        if job is None:
            print 'creating job %s failed!' % n
            continue
        job.id = n
        jobs.append(job)
    cluster.wait()
    cluster.stats()
    cluster.close()
  </code>
<p>
  In the above example, computations send provisional result if
  computed number is <= threshold (0.2). If the number computed is <
  0.005, job_callback deems it acceptable and terminates computations.
  </p>

<h4>NAT/Firewall Forwarding</h4>
<p>
By default dispy client uses UDP and TCP ports 51347, dispynode uses
UDP and TCP ports 51348, and dispyscheduler uses UDP and TCP pots
51347 and TCP port 51348. If client/node/scheduler are behind a NAT
firewall/gateway, then these ports must be forwarded appropriately and
'ext_ip_addr' option must be used. For example, if dispy client is
behdind NAT firewall/gateway, JobCluster/SharedJobCluster must set
'ext_ip_addr' to the NAT firewall/gateway address and forward UDP and
TCP ports 51347 to the IP address where client is running. Similarly,
if dispynode is behind NAT firewall/gateway, 'ext_ip_addr' option must
be used.
  </p>
<p>
  For example, ext_ip_addr feature can be used with Amazon EC2 cloud
  computing service. With EC2 service, a node has a private IP address
  (called 'Private DNS Address') that uses private network of the form
  10.x.x.x and public IP address (called 'Public DNS Address') that is
  of the form ec2-x-x-x-x.x.amazonaws.com. After launching
  instance(s), one can copy dispy files to the node(s) and run
  dispynode as
  <tt>dispynode.py --ext_ip_addr ec2-x-x-x-x.y.amazonaws.com</tt>
  (this address can't be used with
  '-i'/'--ip_addr' option, as the network interface for instance uses
  private IP address only). This node can then be used by dispy client
  by specifying ec2-x-x-x-x.x.amazonaws.com in the 'nodes'
  list. Roughly, dispy uses 'ext_ip_addr' similar to NAT - it
  announces ext_ip_addr to other services instead of the configured
  ip_addr so that external services send requests to ext_ip_addr and
  if firewall/gateway forwards packets appropriately, dispy will
  process them.
  </p>

<h4>Examples</h4>
<p>
  Below are some examples on various use cases of creating clusters:
</p>
  <ol>
    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['node20', '192.168.2.21', 'node24'])</tt>
      sends computation to nodes 'node20', 'node24' and node with IP address '192.168.2.21'; in this case, these nodes could be in different networks, as explicit names / IP addresses are listed.
    </li>
    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.2.*'])</tt>
      sends computation to all nodes whose IP address starts with '192.168.2'.
      In this case, it is assumed that '192.168.2' is local network (since
      dispy can't send identification request to outside networks with wildcard)
    </li>
    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.22', '172.16.11.22', 'node39', '192.168.2.*'])</tt>
      sends computation to nodes with IP addresses '192.168.3.5', '192.168.3.22', '172.16.11.22' and node 'node39' (since explicit names / IP addresses are listed, they could be on different networks), all nodes whose IP address starts with '192.168.2' (local network).
    </li>
    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.*', '192.168.2.*'])</tt>
      In this case, dispy will send identification request to node with IP address '192.168.3.5'. If this node is running 'dispynetrelay', then all the nodes on that network are eligible for executing this computation, as wildcard '192.168.3.*' matches IP addresses of those nodes. In addition, computation is also sent to all nodes whose IP address starts with '192.168.2' (local network).
    </li>
    <li>
      <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.8.20', '172.16.2.99', '*'])</tt>
      In this case, dispy will send identification request to nodes with IP address '192.168.3.5', '192.168.8.20' and '172.16.2.99'. If these nodes all are running dispynetrelay, then all the nodes on those networks are eligible for executing this computation, as wildcard '*' matches IP addresses of those nodes. In addition, computation is also sent to all nodes on local network (since they also match wildcard '*' and identification request is broadcast on local network).
    </li>
    <li>
      Assuming that 192.168.1.39 is the (private) IP address where
      dispy client is used, a.b.c.d is the (public) IP address of NAT
      firewall/gateway (that can be reached from outside) and
      dispynode is running at another public IP address e.f.g.h (so
      that a.b.c.d and e.f.g.h can communicate, but e.f.g.h can't
      communicate with 192.168.1.39),
      <tt>cluster = dispy.JobCluster(compute, ip_addr='192.168.1.39', ext_ip_addr='a.b.c.d', nodes=['e.f.g.h'])</tt>
      would work if NAT firewall/gateway forwards UDP and TCP ports 51347 to
      192.168.1.39.
      </li>
  </ol>

	  </div> 
	</div> 
      </div> 
    </div> 
    <div id="footer"> 
      <p> 
        <a href="http://sourceforge.net/"> 
          Project Web Hosted by <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=539226&amp;type=3" alt="SourceForge.net" /> 
        </a> 
      </p> 
      <p> 
        &copy;Copyright 1999-2009 -
        <a href="http://geek.net" title="Network which provides and promotes Open Source software downloads, development, discussion and news."> 
          Geeknet</a>, Inc., All Rights Reserved
      </p> 
      <p> 
        <a href="http://sourceforge.net/about"> 
          About
        </a> 
        -
        <a href="http://sourceforge.net/tos/tos.php"> 
          Legal
        </a> 
        -
        <a href="http://p.sf.net/sourceforge/getsupport"> 
          Help
        </a> 
      </p> 
    </div> 
  </body> 
</html> 
