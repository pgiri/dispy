<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
  <head> 
    <meta http-equiv="content-type" content="text/html; charset=utf-8" /> 
    <meta name="description" content="Python module for distributing computations across multiple processors on a single machine, among many machines in a cluster, grid or cloud. The computations can be standalone programs or python functions." /> 
    <meta name="keywords" content="dispy, python, parallel processing, distributed computing, cluster computing" /> 
    <title> 
      dispy - Distributed and Parallel Computing with Python
    </title> 

<link rel="stylesheet" type="text/css" href="style.css" />
 
  </head> 
  <body> 
    <div id="page"> 
      <center><div class="title">dispy : Python Framework for Distributed and Parallel Computing</div></center>
      <div id="menu">
	<ul>
          <li><a href="http://sourceforge.net/projects/dispy/files">Download</a></li> 
          <li><a href="http://sourceforge.net/projects/dispy/">Project Details</a></li> 
          <li><a href="http://sourceforge.net/p/dispy/discussion">Forums</a></li>
	</ul>
	<hr />
	<ul>
          <li><a href="index.html">Project Page</a></li> 
          <li><a href="dispy.html">dispy</a></li> 
          <li><a href="dispynode.html">dispynode</a></li> 
          <li><a href="dispyscheduler.html">dispyscheduler</a></li> 
          <li><a href="dispynetrelay.html">dispynetrelay</a></li> 
	</ul>
      </div> 

    <div id="content"> 
<center><h2>dispy Examples</h2></center>

<p>dispy can be used to distribute programs or Python program
fragments to nodes and execute them in parallel. It supports various
options to handle rather comprehensive cases (such as fault
tolerance, sharing nodes in multiple programs simulataneously, using
nodes in multiple networks etc.); however, in common setups, the usage
is simple, as done in the demonstrative examples below.
  </p>

<ol>
  <li>A simple script that distributes a program (say,
  '/path/to/program') to all the nodes in a local network running
  <a href="dispynode.html">dispynode</a>, executes them with a
  sequence of numbers is:
<pre><code>import dispy
cluster = dispy.JobCluster('/path/to/program')
for i in range(50):
    cluster.submit(i)</code></pre>

<p>
The program '/path/to/program' on the client computer is transferred
to each of the nodes, so if the program is a binary program then all
the nodes should have same architecture as the client.</p>
<p> In this case we assume that the programs execute and save the
computation results in a database, file system etc. If we are
interested in exit status, output from each run etc., then we need
to collect each of the jobs submitted from which interested attributes
can be retrieved, as done in the example below.</p>
    </li>

  <li>A canonical cluster that distributes computation 'compute'
  (Python function) to nodes running <a href="dispynode.html">dispynode</a> on a local
  network, schedules jobs with the cluster, gets jobs' results and
  prints them is:
<pre><code># function 'compute' is distributed and executed with arguments
# supplied with 'cluster.submit' below
def compute(n):
    import time, socket
    time.sleep(n)
    host = socket.gethostname()
    return (host, n)

if __name__ == '__main__':
    import dispy, random
    cluster = dispy.JobCluster(compute)
    jobs = []
    for n in range(20):
        job = cluster.submit(random.randint(5,20))
        job.id = n
        jobs.append(job)
    # cluster.wait()
    for job in jobs:
        host, n = job() # waits for job to finish and returns results
        print '%s executed job %s at %s with %s' % (host, job.id, job.start_time, n)
        # other fields of 'job' that may be useful:
        # job.stdout, job.stderr, job.exception, job.ip_addr, job.end_time
    cluster.stats()
</code>
</pre>
<p>If the computation has any dependencies, such as classes, objects
or files, they can be specified with 'depends' argument and dispy will
distribute them along with the computation.
  </p>
</li>

<li>Continuing futile but illustrative examples, the program below
distributes computation to be executed with instances of a class.
<pre><code>class C:
    def __init__(self, i, n):
        self.i = i
        self.n = n

    def show(self):
        print('%s: %.2f' % (self.i, self.n))

def compute(obj):
    # obj is an instance of C
    import time
    time.sleep(obj.n)
    obj.show()
    return obj.n

if __name__ == '__main__':
    import random, dispy
    cluster = dispy.JobCluster(compute, depends=[C])
    jobs = []
    for i in range(10):
        c = C(i, random.uniform(1, 3)) # create object of C
        job = cluster.submit(c) # it is sent to a node for executing 'compute'
        job.id = c # store this object for later use
        jobs.append(job)
    for job in jobs:
        job() # wait for job to finish
        print('%s: %.2f / %s' % (job.id.i, job.result, job.stdout))
</code></pre>
<p>Note that class C is given in 'depends' so the code for it is
transferred to the nodes automatically, so that the objects created in
client program work transparently in 'compute' on remote nodes. The
objects are serialized using Pickle and sent over the to the nodes, so
the objects must be serializable. If they are not serializable (e.g.,
they contain references to locks), then the class must
provide <strong>__getstate__</strong>
and <strong>__setstate__</strong> methods;
see <a href="http://docs.python.org/2/library/pickle.html">Python
object serialization</a> for details. In addition, the objects
shouldn't contain file descriptors, references to other objects not
being transferred etc., which are not valid on remote nodes.
  </p>
  </li>

<li>
  <p>'setup' and 'cleanup' parameters can be used to set / unset global
  variables on a node. This feature works with Linux, OS X and other Unix
  variants (but not Windows) where child processes (that run
  computation jobs) share address space (for read-only purposes) with
  parent (dispynode program where 'setup' function is executed). In
  the example below, data in file "file.dat" is read and stored in
  global variable in 'setup' function. The jobs work on the data in
  the global variable. The 'cleanup' function deletes the global
  variable.</p>
 <pre><code>def setup():
    # read data in file to global variable
    global data
    data = open('file.dat').read()
    return 0

def cleanup():
    del globals()['data']

def compute(n):
    import hashlib
    alg = hashlib.algorithms[n % len(hashlib.algorithms)]
    csum = getattr(hashlib, alg)()
    # 'data' global variable has file data
    csum.update(data)
    return (alg, csum.hexdigest())

if __name__ == '__main__':
    import dispy
    cluster = dispy.JobCluster(compute, depends=['file.dat'], setup=setup, cleanup=cleanup)
    jobs = []
    for n in range(10):
        job = cluster.submit(n)
        job.id = n
        jobs.append(job)

    for job in jobs:
        job()
        print('%s: %s : %s' % (job.id, job.result[0], job.result[1]))
</code></pre>
  </li>

<li>
  <p>'setup' and 'cleanup' can also be used to create shared variables
  so jobs executing on a node can share state (in a limited way, using
  multiprocessing module's shared memory support). The program below
  creates an integer shared variable that is updated by jobs running on
  <em>that node</em>.</p> Note that, as mentioned above, under Windows
  child processes don't inherit address space from parent process, so
  global variables created in 'setup' function will not be available
  in job function.
<pre><code>def setup():
    import multiprocessing, multiprocessing.sharedctypes
    # import 'random' into global scope, create global shared variable
    global random, shvar
    import random
    lock = multiprocessing.Lock()
    shvar = multiprocessing.sharedctypes.Value('i', 1, lock=lock)
    return 0

def cleanup():
    del globals()['shvar']
    # unload 'random' module (doesn't undo everything import does)
    del sys.modules['random']
    del globals()['random']

def compute():
    r = random.randint(1, 10)
    global shvar
    shvar.value += r
    return shvar.value

if __name__ == '__main__':
    import dispy
    cluster = dispy.JobCluster(compute, setup=setup, cleanup=cleanup)
    jobs = []
    for n in range(10):
        job = cluster.submit()
        job.id = n
        jobs.append(job)

    for job in jobs:
        job()
        if job.status != dispy.DispyJob.Finished:
            print('job %s failed: %s' % (job.id, job.exception))
        else:
            print('%s: %s' % (job.id, job.result))
</code></pre>
  </li>

</ol>

<p>
This framework can be customized for various use cases; some examples
are:
</p>
<ol>
  <li><tt>cluster = dispy.JobCluster(compute, depends=[ClassA, moduleB, 'file1'])</tt><br />
    distributes 'compute' along with ClassA (Python object), moduleB (Python object) and
    'file1', a file on client computer. Presumably ClassA, moduleB and file1
    are needed by 'compute'.</li>

  <li><tt>cluster = dispy.JobCluster(compute, nodes=['node20', '192.168.2.21', 'node24'])</tt>
    sends computation to nodes 'node20', 'node24' and node with IP address '192.168.2.21'.
    These nodes could be in different networks, as explicit names / IP addresses
    are listed.</li>

  <li>If nodes are on remote network, then certain ports need to be
    forwarded as the nodes connect to the client to send status / results of
    jobs; see <a href="dispy.html#NAT">NAT/Firewall Forwarding
      section</a> in <a href="dispy.html">dispy</a>. If port forwarding
    is not possible, then ssh tunneling can be used. To use this,
    ssh to each node with
    <tt>ssh -R 51347:127.0.0.1:51347 node</tt> (to possibly
    execute <a href="dispynode.html">dispynode</a> on the node if not already running), then
    specify 'ip_addr=127.0.0.1' to dispy.JobCluster; dispy issues a
    warning about using localhost address, but in this case the
    warning is harmless.
  </li>

  <li>
    <tt>cluster = dispy.JobCluster(compute, nodes=['192.168.2.*'])</tt>
    sends computation to all nodes whose IP address starts with '192.168.2'.
    In this case, it is assumed that '192.168.2' is local network (since
    UDP broadcast is used to discover nodes in a network and broadcasting packets
    don't cross networks).
  </li>

  <li>
    <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.22',
                                         '172.16.11.22', 'node39', '192.168.2.*'])</code>
    sends computation to nodes with IP addresses '192.168.3.5', '192.168.3.22', '172.16.11.22' and node 'node39' (since explicit names / IP addresses are listed, they could be on different networks), all nodes whose IP address starts with '192.168.2' (local network).
    </li>

  <li>
    <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.3.*', '192.168.2.*'])</code>
    In this case, dispy will send identification request to node with IP address '192.168.3.5'.
    If this node is running 'dispynetrelay', then all the nodes on that network are eligible for
    executing this computation, as wildcard '192.168.3.*' matches IP addresses of those nodes.
    In addition, computation is also sent to all nodes whose IP address starts with '192.168.2'
    (local network).
  </li>

  <li>
    <code>cluster = dispy.JobCluster(compute, nodes=['192.168.3.5', '192.168.8.20',
                                          '172.16.2.99', '*'])</code>
    In this case, dispy will send identification request to nodes with IP address '192.168.3.5',
    '192.168.8.20' and '172.16.2.99'. If these nodes all are running dispynetrelay, then all
    the nodes on those networks are eligible for executing this computation, as wildcard '*'
    matches IP addresses of those nodes. In addition, computation is also sent to all nodes on
    local network (since they also match wildcard '*' and identification request is broadcast
    on local network).
  </li>

  <li>
    Assuming that 192.168.1.39 is the (private) IP address where
    dispy client is used, a.b.c.d is the (public) IP address of NAT
    firewall/gateway (that can be reached from outside) and
    dispynode is running at another public IP address e.f.g.h (so
    that a.b.c.d and e.f.g.h can communicate, but e.f.g.h can't
    communicate with 192.168.1.39),<br />
    <code>cluster = dispy.JobCluster(compute, ip_addr='192.168.1.39', ext_ip_addr='a.b.c.d',
                               nodes=['e.f.g.h'])</code>
    would work if NAT firewall/gateway forwards UDP and TCP ports 51347 to
    192.168.1.39.
  </li>

  <li><tt>cluster = dispy.JobCluster(compute, secret='super') </tt><br />
    distributes 'compute' to nodes that also use secret 'super' (i.e.,
    nodes started with 'dispynode.py -s super')<br/> Note that secret is
    used only for establishing communication initially, but not used
    to encrypt programs or code for python objects. This can be useful
    to prevent other users from (inadvertantly) using the nodes. If
    encryption is needed, use SSL; see below.</li>

  <li><tt>cluster = dispy.JobCluster(compute, certfile='mycert',
      keyfile='mykey')</tt><br /> distributes 'compute' and encrypts all
    communication using SSL certificate stored in 'mycert' file and key
    stored in 'mykey' file. In this case, dispynode must also use same
    certificate and key; i.e., each dispynode must be invoked with
    <tt>dispynode --certfile="mycert" --keyfile="mykey"'</tt><br/>

    If both certificate and key are stored in same file, say,
    'mycertkey', they are expected to be in certfile:<br />
    <tt>cluster = dispy.JobCluster(compute, certfile='mycertkey')</tt></li>

  <li><tt>cluster1 = dispy.JobCluster(compute1, nodes=['192.168.3.2', '192.168.3.5'])</tt><br/>
    <tt>cluster2 = dispy.JobCluster(compute2, nodes=['192.168.3.10',
      '192.168.3.11'])</tt><br /> distributes 'compute1' to nodes
    192.168.3.2 and 192.168.3.5, and 'compute2' to nodes 192.168.3.10
    and 192.168.3.11. With this setup, specific computations can be
    scheduled on certain node(s). As mentioned above, with JobCluster,
    the set of nodes for one cluster must be disjoint with set of
    nodes in any other cluster running at the same time. Otherwise,
    <a href="dispy.html">SharedJobCluster</a> must be used.</li>

</ol>

<p>
    A simple version of <a href="http://en.wikipedia.org/wiki/MapReduce">word count example from MapReduce</a>:
    <pre><code># a version of word frequency example from mapreduce tutorial

def mapper(doc):
    # input reader and map function are combined
    import os
    words = []
    with open(os.path.join('/tmp', doc)) as fd:
        for line in fd:
            words.extend((word.lower(), 1) for word in line.split() \
                         if len(word) > 3 and word.isalpha())
    return words

def reducer(words):
    # we should generate sorted lists which are then merged,
    # but to keep things simple, we use dicts
    word_count = {}
    for word, count in words:
        if word not in word_count:
            word_count[word] = 0
        word_count[word] += count
    # print 'reducer: %s to %s' % (len(words), len(word_count))
    return word_count

if __name__ == '__main__':
    import dispy, logging
    # map
    # nodes node1 and node2 have 'doc1', 'doc2' etc. on their
    # local storage under /tmp, so no need to transfer them
    map_cluster = dispy.JobCluster(mapper, nodes=['node1', 'node2'], pulse_interval=2,
                                   reentrant=True)
    reduce_cluster = dispy.JobCluster(reducer, nodes=['*'], pulse_interval=2,
                                      reentrant=True)
    map_jobs = []
    for f in ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']:
        job = map_cluster.submit(f)
        map_jobs.append(job)
    reduce_jobs = []
    for map_job in map_jobs:
        words = map_job()
        if not words:
            print map_job.exception
            continue
        # simple partition
        n = 0
        while n < len(words):
            m = min(len(words) - n, 1000)
            reduce_job = reduce_cluster.submit(words[n:n+m])
            reduce_jobs.append(reduce_job)
            n += m
    # reduce
    word_count = {}
    for reduce_job in reduce_jobs:
        words = reduce_job()
        if not words:
            print reduce_job.exception
            continue
        for word, count in words.iteritems():
            if word not in word_count:
                word_count[word] = 0
            word_count[word] += count
    # sort words by frequency and print
    for word in sorted(word_count, key=lambda x: word_count[x], reverse=True):
        count = word_count[word]
        print word, count
    reduce_cluster.stats()

      </code></pre>

</p>

      </div> 
    </div> 
    <div id="footer"> 
      <p> 
        <a href="http://sourceforge.net/"> 
          Project Web Hosted by <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=539226&amp;type=3" alt="SourceForge.net" /> 
        </a> 
      </p> 
      <p> 
        &copy;Copyright 1999-2009 -
        <a href="http://geek.net" title="Network which provides and promotes Open Source software downloads, development, discussion and news."> 
          Geeknet</a>, Inc., All Rights Reserved
      </p> 
      <p> 
        <a href="http://sourceforge.net/about"> 
          About
        </a> 
        -
        <a href="http://sourceforge.net/tos/tos.php"> 
          Legal
        </a> 
        -
        <a href="http://p.sf.net/sourceforge/getsupport"> 
          Help
        </a> 
      </p> 
    </div> 

  </body> 
</html> 
